{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tracks = pickle.load(open('data/all_session_tracks_train.pkl','rb'))\n",
    "test_tracks = pickle.load(open('data/all_session_tracks_test.pkl','rb'))\n",
    "train_skips = pickle.load(open('data/all_session_skips_train.pkl','rb'))\n",
    "test_skips = pickle.load(open('data/all_session_skips_test.pkl','rb'))\n",
    "track_vocab = pickle.load(open('data/track_vocabs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.8*len(train_tracks))\n",
    "train_tracks, valid_tracks, train_skips, valid_skips =  train_tracks[0:split_index],train_tracks[split_index:],\\\n",
    "train_skips[0:split_index],train_skips[split_index:]\n",
    "test_tracks, test_skips = test_tracks[:10000], test_skips[:10000]\n",
    "train_tracks, train_skips = train_tracks[:100000], train_skips[:100000]\n",
    "valid_tracks, valid_skips = valid_tracks[:1000], valid_skips[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = np.load(open('data/track_embedding.npy','rb')).astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = torch.from_numpy(normalize(track_embeddings,axis=0)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0031,  0.0031,  0.0031,  ...,  0.0031, -0.0031, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0031,  0.0031,  0.0031,  ...,  0.0031, -0.0031, -0.0031],\n",
       "        [ 0.0031,  0.0031,  0.0031,  ...,  0.0031, -0.0031, -0.0031],\n",
       "        [ 0.0031,  0.0031,  0.0031,  ...,  0.0031, -0.0031, -0.0031]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10000, 100000, 10000, 103910, torch.Size([103910, 26]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tracks), len(test_tracks), len(train_skips), len(test_skips), len(track_vocab), track_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for tracks,labels in zip(train_tracks,train_skips):\n",
    "    doc = []\n",
    "    for track,label in zip(tracks,labels):\n",
    "        if label == 1:\n",
    "            doc.append(str(track))\n",
    "    if len(doc) > 0:\n",
    "        docs.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=docs, size=100, window=10, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.wv[model.wv.vocab]\n",
    "X = X[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = np.load(open('data/track_embedding.npy','rb')).astype('double')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = track_embeddings.shape\n",
    "track_embeddings_new = np.zeros((r,c+100)).astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-225-0ebec5608160>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  track_embeddings_new[i,c:] = model[str(i)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(r):\n",
    "    try:\n",
    "        track_embeddings_new[i,:c] =  track_embeddings[i]\n",
    "        track_embeddings_new[i,c:] = model[str(i)]\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.55600006e+02,  1.98200000e+03,  9.98926604e+01,  1.50385752e-01,\n",
       "        7.02139378e-01,  7.04185188e-01,  6.40115798e-01,  9.79656506e+00,\n",
       "        2.55722582e-01,  1.01361072e+00,  1.45051544e-07,  7.00000000e+00,\n",
       "        5.21661043e-02, -1.76949997e+01,  5.88657856e-01,  3.97264026e-02,\n",
       "        8.46669998e+01,  4.00000000e+00,  8.37420762e-01,  7.34024197e-02,\n",
       "       -1.63914576e-01, -2.19623372e-01, -3.72237824e-02,  6.84699357e-01,\n",
       "       -2.19794884e-02, -2.66232938e-01,  7.32988771e-03, -7.86207733e-04,\n",
       "       -1.02029517e-02,  4.70242873e-02,  1.45070814e-02, -7.13560283e-02,\n",
       "        1.71769112e-02,  3.84193882e-02,  5.02474830e-02,  4.22528014e-02,\n",
       "        1.14100417e-02, -2.78412383e-02, -9.19252355e-03,  6.33520931e-02,\n",
       "       -4.67117131e-02,  4.07424271e-02, -4.76609319e-02, -5.18641658e-02,\n",
       "        3.53294872e-02,  2.35150717e-02,  5.49882241e-02, -5.66939032e-03,\n",
       "        8.36028084e-02, -2.42765751e-02, -6.53053820e-02, -1.33848796e-02,\n",
       "        2.01201178e-02,  9.49635450e-03,  1.06846923e-02,  6.02823356e-03,\n",
       "       -2.81576551e-02, -4.93507609e-02,  5.05351275e-02, -6.41728118e-02,\n",
       "        1.48070985e-02,  1.64074246e-02,  1.56325437e-02, -4.56618890e-02,\n",
       "        4.89239916e-02,  5.83821647e-02, -6.69735000e-02,  2.31440575e-03,\n",
       "       -5.86194247e-02, -2.62453035e-02, -1.80740841e-02,  2.28155088e-02,\n",
       "       -1.98872644e-03, -4.48242947e-02, -2.86088325e-02, -7.56340707e-03,\n",
       "       -8.17741156e-02,  6.15250692e-02, -2.45708469e-02,  4.03085798e-02,\n",
       "        9.88120586e-03,  4.78740744e-02, -6.06650785e-02, -5.34979962e-02,\n",
       "        9.88259837e-02,  2.27672774e-02,  1.80949625e-02,  3.84010337e-02,\n",
       "       -3.29614792e-04, -2.94576958e-02,  6.25496954e-02,  2.01384481e-02,\n",
       "       -6.22866936e-02, -5.80768799e-03,  3.44426967e-02,  2.71526705e-02,\n",
       "        7.29256496e-02,  2.85133645e-02, -6.55778265e-03, -6.08390430e-03,\n",
       "       -6.82214499e-02, -2.21060999e-02, -8.09361637e-02, -5.85696138e-02,\n",
       "       -7.93041587e-02, -6.62521124e-02, -1.02247009e-02, -1.50573738e-02,\n",
       "        5.02547286e-02, -8.39722995e-03,  2.78184302e-02,  7.54260421e-02,\n",
       "       -4.00862284e-02,  1.14767961e-02, -5.61950430e-02,  3.05220892e-04,\n",
       "        3.63320229e-03,  6.59223795e-02,  3.94991897e-02,  3.36339064e-02,\n",
       "       -6.76764622e-02, -1.16319070e-02,  3.28625366e-02, -4.68701012e-02,\n",
       "        1.47069967e-03,  1.61428303e-01])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings_new[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/track_embeddings_w2v', track_embeddings_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = np.load(open('data/track_embeddings_w2v.npy','rb')).astype('double')\n",
    "track_embeddings = torch.from_numpy(normalize(track_embeddings,axis=0)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103910, 126])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.1022e-03,  3.1022e-03,  3.1022e-03,  3.1022e-03,  3.1022e-03,\n",
       "         3.1022e-03,  3.1022e-03,  3.1022e-03,  3.1022e-03,  3.1022e-03,\n",
       "         3.1022e-03,  3.1022e-03,  3.1022e-03, -3.1022e-03,  3.1022e-03,\n",
       "         3.1022e-03,  3.1022e-03,  3.1022e-03,  3.1022e-03,  3.1022e-03,\n",
       "        -3.1022e-03, -3.1022e-03, -3.1022e-03,  3.1022e-03, -3.1022e-03,\n",
       "        -3.1022e-03,  2.7058e-04, -2.9573e-05, -4.1658e-04,  9.6330e-04,\n",
       "         3.5781e-04, -9.5962e-04,  4.0126e-04,  7.8329e-04,  1.2530e-03,\n",
       "         8.9054e-04,  4.7037e-04, -7.9674e-04, -3.2201e-04,  1.2973e-03,\n",
       "        -8.9846e-04,  1.0141e-03, -9.7301e-04, -1.1064e-03,  9.9643e-04,\n",
       "         3.8019e-04,  1.0997e-03, -1.7908e-04,  1.2248e-03, -5.4952e-04,\n",
       "        -1.0260e-03, -3.2037e-04,  4.6941e-04,  2.5603e-04,  4.8005e-04,\n",
       "         1.7260e-04, -7.2382e-04, -1.0119e-03,  1.1882e-03, -1.2271e-03,\n",
       "         3.6836e-04,  5.6691e-04,  5.4309e-04, -1.0933e-03,  7.2565e-04,\n",
       "         1.3069e-03, -1.1817e-03,  4.4540e-05, -1.2050e-03, -8.0959e-04,\n",
       "        -6.4337e-04,  5.7875e-04, -7.8883e-05, -9.2252e-04, -6.5169e-04,\n",
       "        -2.5096e-04, -1.3621e-03,  1.2314e-03, -7.5251e-04,  8.9577e-04,\n",
       "         3.6329e-04,  1.0149e-03, -9.0627e-04, -1.2580e-03,  1.2844e-03,\n",
       "         5.4403e-04,  7.4150e-04,  7.8724e-04, -8.1854e-06, -7.5174e-04,\n",
       "         1.2312e-03,  6.4543e-04, -1.2664e-03, -1.4382e-04,  6.6887e-04,\n",
       "         8.9919e-04,  1.3071e-03,  6.5690e-04, -3.3931e-04, -1.6034e-04,\n",
       "        -1.2779e-03, -5.5111e-04, -1.0858e-03, -1.4344e-03, -1.2392e-03,\n",
       "        -1.4323e-03, -5.7485e-04, -4.5840e-04,  8.8952e-04, -2.8140e-04,\n",
       "         7.1217e-04,  9.7152e-04, -1.0646e-03,  4.2573e-04, -1.2521e-03,\n",
       "         9.3852e-06,  1.0878e-04,  1.2248e-03,  8.4192e-04,  8.7927e-04,\n",
       "        -1.2269e-03, -3.9087e-04,  7.4107e-04, -1.0041e-03,  5.6844e-05,\n",
       "         1.3019e-03], dtype=torch.float64)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = track_embeddings_new\n",
    "track_embeddings = torch.from_numpy(normalize(track_embeddings,axis=0)).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103910, 26])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSpotify(Dataset):\n",
    "    \n",
    "    def __init__(self, tracks, skips, transform=None):\n",
    "        self.tracks = tracks\n",
    "        self.skips = skips\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tracks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        features = self.tracks[index]\n",
    "        label = self.skips[index]\n",
    "        \n",
    "#         features_new = []\n",
    "#         for x,y in zip(features[:10],label[:10]):\n",
    "#             if y == 1:\n",
    "#                 features_new.append(x)\n",
    "#             else:\n",
    "#                 features_new.append(0)\n",
    "#         features_new.extend(features[10:])\n",
    "\n",
    "#         features = features_new\n",
    "\n",
    "#         return np.array(features[:10]), np.array(features[10:])\n",
    "    \n",
    "        return np.array(features), np.array(label)\n",
    "#         return torch.from_numpy(np.array(features)), torch.from_numpy(np.array(label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetSpotify(train_tracks,train_skips)\n",
    "valid_dataset = DatasetSpotify(valid_tracks,valid_skips)\n",
    "test_dataset = DatasetSpotify(test_tracks,test_skips)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[103302,  51333,  72014,  ...,  74387,  14816, 102419],\n",
       "         [ 22430,  22430,  56612,  ...,  64317,  23134,  42242],\n",
       "         [ 59731,  20787, 101918,  ...,  77837,   7383,  83707],\n",
       "         ...,\n",
       "         [ 47617,  33642,  20492,  ...,   6768, 102631,  47220],\n",
       "         [ 42532,  46678,  57173,  ...,  73602,  10058,  28902],\n",
       "         [ 67462,  91639,  71969,  ...,  52180,  33642,  36692]]),\n",
       " tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 0, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [0, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Seq2Seq Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(track_embeddings)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src)).float()\n",
    "        \n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(track_embeddings)\n",
    "\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input)).float()\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Seq2Seq1(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        split_index = math.floor(src.shape[0]//2)\n",
    "\n",
    "        seq_to_encode = src[0:split_index,:]\n",
    "        seq_to_decode = src[split_index:,:]\n",
    "        targets_of_decoded = trg\n",
    "        \n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = targets_of_decoded.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(seq_to_encode)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        input = trg[split_index-1,:]\n",
    "        \n",
    "        for t in range(trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "#             teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arorasagar/miniconda3/envs/cs7643-a1/lib/python3.8/site-packages/torch/nn/modules/rnn.py:57: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 256, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=256\n",
    "INPUT_DIM = len(track_vocab) \n",
    "OUTPUT_DIM = 2\n",
    "# OUTPUT_DIM = len(track_vocab) \n",
    "\n",
    "ENC_EMB_DIM = track_embeddings.shape[1]\n",
    "DEC_EMB_DIM = track_embeddings.shape[1]\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq1(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "# print(next(train_iter)[0])\n",
    "\n",
    "x,y = next(train_iter)\n",
    "model.forward(x.permute(1,0),y.permute(1,0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(tracks_dict) + 1\n",
    "# OUTPUT_DIM = 2\n",
    "# ENC_EMB_DIM = 26\n",
    "# DEC_EMB_DIM = 26\n",
    "# HID_DIM = 512\n",
    "# N_LAYERS = 1\n",
    "# ENC_DROPOUT = 0.2\n",
    "# DEC_DROPOUT = 0.2\n",
    "\n",
    "# enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "# dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# model = Seq2Seq1(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq1(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(103910, 26)\n",
       "    (rnn): LSTM(26, 512, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(103910, 26)\n",
       "    (rnn): LSTM(26, 512, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,212,866 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DatasetSpotify(train_df)\n",
    "# valid_dataset = DatasetSpotify(validation_df)\n",
    "# test_dataset = DatasetSpotify(test_ddf)\n",
    "\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 256 \n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    total_maa = 0\n",
    "    \n",
    "    for i, (x,y) in enumerate(iterator):\n",
    "        \n",
    "        src = x.permute(1,0)\n",
    "#         print('shape is',src.shape)\n",
    "        trg = y.permute(1,0)\n",
    "#         print('shape is',src.shape, trg.shape)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        split_index = math.floor(src.shape[0]//2)\n",
    "\n",
    "       \n",
    "        trg = trg[split_index:,:]\n",
    "\n",
    "        \n",
    "        output = model(src, trg, 0)\n",
    "        maa = mean_average_accuracy(output.permute(1,0,2),trg.permute(1,0))\n",
    "\n",
    "        \n",
    "#         print('now')\n",
    "#         print(output.shape, trg.shape)\n",
    "#         print(output[:,0,:],trg[:,0])\n",
    "#         print(output[:,1,:],trg[:,1])\n",
    "#         print(output[:,2,:],trg[:,2])\n",
    "#         print(output[:,3,:],trg[:,3])\n",
    "#         break\n",
    "\n",
    "        \n",
    "#         if i % 100 == 0:\n",
    "#             print('Loss',i,epoch_loss)\n",
    "#             print('mean avg accuracy', mean_average_accuracy(output.permute(1,0,2)[10:],trg.permute(1,0)[10:]))\n",
    "\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "#         print('here initial output shape is ',output.shape)\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "#         print('output shapes are',output.shape, trg.shape)\n",
    "\n",
    "        trg = trg[1:].reshape(-1)\n",
    "    \n",
    "#         print('output is ',output)\n",
    "#         print('trg is ',trg)\n",
    "\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "#         print(maa, loss)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        total_maa += maa\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator), total_maa / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    total_maa = 0\n",
    "    total_acc = torch.zeros(10)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (x,y) in enumerate(iterator):\n",
    "\n",
    "            src = x.permute(1,0)\n",
    "            trg = y.permute(1,0)\n",
    "            split_index = math.floor(src.shape[0]//2)\n",
    "\n",
    "       \n",
    "            trg = trg[split_index:,:]\n",
    "\n",
    "        \n",
    "            output = model(src, trg, 0)\n",
    "\n",
    "#             output = model(src, trg, 0) #turn off teacher forcing\\\n",
    "            max_output = torch.argmax(output,dim=2)\n",
    "            maa = mean_average_accuracy(output.permute(1,0,2),trg.permute(1,0))\n",
    "            local_acc = accuracy_at_k(output.permute(1,0,2),trg.permute(1,0))\n",
    "            \n",
    "#             print(output.shape, max_output.permute(1,0).reshape(-1,20).shape,trg.permute(1,0).reshape(-1,20).shape)\n",
    "#             if i % 100 == 0:\n",
    "#                 maa = mean_average_accuracy(output.permute(1,0,2)[10:],trg.permute(1,0)[10:])\n",
    "#                 print('iter ',i)\n",
    "# #                 print(max_output[:10][10:])\n",
    "#                 print('acc is', torch.mean(accuracy(max_output.permute(1,0).reshape(-1,20),trg.permute(1,0).reshape(-1,20))))\n",
    "#                 print('mean avg accuracy', maa)\n",
    "\n",
    "\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total_maa += maa\n",
    "            total_acc += local_acc\n",
    "            \n",
    "        \n",
    "    return epoch_loss / len(iterator), total_maa / len(iterator) , total_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    \n",
    "    \"\"\"Computes the accuracy of the predicted skip sequence\"\"\"\n",
    "    \n",
    "    seq_len = target.shape[1]\n",
    "    correct = output.eq(target)\n",
    "    correct = correct.sum(axis=1) * 1.0\n",
    "    acc = correct / seq_len\n",
    "    return acc\n",
    "def mean_average_accuracy(output, target):\n",
    "    \n",
    "    \"\"\"Computes the  mean average accuracy of the predicted skip sequence up till the 10th seq position\"\"\"\n",
    "    output = output.to(device)\n",
    "    target = target.to(device)\n",
    "    T = output.shape[1]\n",
    "    batch_size = target.shape[0]\n",
    "    output = torch.argmax(output, dim=2)\n",
    "    A_i = torch.zeros(batch_size, T)\n",
    "    L_i = torch.zeros(batch_size, T)\n",
    "#     print('T ',T)\n",
    "#     print(output[:10,0:])\n",
    "#     print(target[:10,0:])\n",
    "\n",
    "    for i in range(1,T+1):\n",
    "        \n",
    "        A_i[:,i-1] = accuracy(output[:,0:i], target[:,0:i])\n",
    "        pred_i = output[:,i-1]\n",
    "        target_i = target[:,i-1]\n",
    "        L_i[:,i-1] = pred_i.eq(target_i)*1.0\n",
    "        A_i[:,i-1] = A_i[:,i-1]*L_i[:,i-1]\n",
    "    AA = A_i.sum(dim =1) / T\n",
    "    return torch.sum(AA) / batch_size\n",
    "def accuracy_at_k(output, target):\n",
    "    \n",
    "    \"\"\"Computes the  mean average accuracy of the predicted skip sequence up till the 10th seq position\"\"\"\n",
    "    output = output.to(device)\n",
    "    target = target.to(device)\n",
    "#     print(output.shape)    \n",
    "#     print(target.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    T = output.shape[1]\n",
    "#     print(T)\n",
    "    batch_size = target.shape[0]\n",
    "    output = torch.argmax(output, dim=2)\n",
    "    A_i = torch.zeros(batch_size, T)\n",
    "    L_i = torch.zeros(batch_size, T)\n",
    "#     print('T ',T)\n",
    "#     print(output[:10,0:])\n",
    "#     print(target[:10,0:])\n",
    "    acc = torch.zeros(T)\n",
    "\n",
    "    for i in range(T):\n",
    "#         print(output[:,i].shape)\n",
    "        \n",
    "        acc[i] = torch.mean(accuracy(output[:,i].reshape(batch_size,1), target[:,i].reshape(batch_size,1)))\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 15s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758 | Train MAA:   0.518\n",
      "\t Val. Loss: 0.562 |  Val. PPL:   1.754 | Valid MAA:   0.522\n",
      "Epoch: 02 | Time: 2m 15s\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756 | Train MAA:   0.519\n",
      "\t Val. Loss: 0.562 |  Val. PPL:   1.754 | Valid MAA:   0.522\n",
      "Epoch: 03 | Time: 2m 30s\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.755 | Train MAA:   0.520\n",
      "\t Val. Loss: 0.562 |  Val. PPL:   1.754 | Valid MAA:   0.522\n",
      "Epoch: 04 | Time: 2m 57s\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.755 | Train MAA:   0.520\n",
      "\t Val. Loss: 0.562 |  Val. PPL:   1.754 | Valid MAA:   0.522\n",
      "Epoch: 05 | Time: 3m 17s\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.755 | Train MAA:   0.520\n",
      "\t Val. Loss: 0.562 |  Val. PPL:   1.753 | Valid MAA:   0.522\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_maa =  train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_maa, _ = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'vanilla.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train MAA: {train_maa:7.3f}' )\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Valid MAA: {valid_maa:7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.563 | Test PPL:   1.755 | Test MAA:   0.521\n",
      "test acc,  tensor([0.5720, 0.6030, 0.6092, 0.6222, 0.6309, 0.6443, 0.6594, 0.6922, 0.7594,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100000 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('vanilla.pt'))\n",
    "\n",
    "test_loss, test_maa, test_acc = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test MAA: {test_maa:7.3f}')\n",
    "print('test acc, ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Dataset to modify skipped track with skip token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSpotify(Dataset):\n",
    "    \n",
    "    def __init__(self, tracks, skips, transform=None):\n",
    "        self.tracks = tracks\n",
    "        self.skips = skips\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tracks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        features = self.tracks[index]\n",
    "        label = self.skips[index]\n",
    "        \n",
    "        features_new = []\n",
    "        for x,y in zip(features[:10],label[:10]):\n",
    "            if y == 1:\n",
    "                features_new.append(x)\n",
    "            else:\n",
    "                features_new.append(0)\n",
    "        features_new.extend(features[10:])\n",
    "\n",
    "        features = features_new\n",
    "\n",
    "#         return np.array(features[:10]), np.array(features[10:])\n",
    "    \n",
    "        return np.array(features), np.array(label)\n",
    "#         return torch.from_numpy(np.array(features)), torch.from_numpy(np.array(label))\n",
    "    \n",
    "\n",
    "train_dataset = DatasetSpotify(train_tracks,train_skips)\n",
    "valid_dataset = DatasetSpotify(valid_tracks,valid_skips)\n",
    "test_dataset = DatasetSpotify(test_tracks,test_skips)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 75651,      0,      0,  ...,  78207,  67988,  67988],\n",
       "         [     0,      0,      0,  ...,  66049, 102212, 101661],\n",
       "         [ 50497,  67863,  77965,  ...,  40205,  69471,  67987],\n",
       "         ...,\n",
       "         [     0,      0,      0,  ...,  67462,  12808,  52543],\n",
       "         [ 43011,    379,   6921,  ..., 103405,  90487,  16457],\n",
       "         [ 28669,      0,      0,  ...,  52209,  32783,  75824]]),\n",
       " tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 0],\n",
       "         [1, 0, 0,  ..., 1, 1, 0]])]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 52s\n",
      "\tTrain Loss: 0.536 | Train PPL:   1.709 | Train MAA:   0.592\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694 | Valid MAA:   0.608\n",
      "Epoch: 02 | Time: 2m 54s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.696 | Train MAA:   0.606\n",
      "\t Val. Loss: 0.525 |  Val. PPL:   1.690 | Valid MAA:   0.611\n",
      "Epoch: 03 | Time: 2m 48s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695 | Train MAA:   0.606\n",
      "\t Val. Loss: 0.525 |  Val. PPL:   1.691 | Valid MAA:   0.608\n",
      "Epoch: 04 | Time: 2m 32s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695 | Train MAA:   0.606\n",
      "\t Val. Loss: 0.525 |  Val. PPL:   1.691 | Valid MAA:   0.609\n",
      "Epoch: 05 | Time: 2m 24s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694 | Train MAA:   0.605\n",
      "\t Val. Loss: 0.526 |  Val. PPL:   1.691 | Valid MAA:   0.606\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_maa =  train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_maa, _ = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'vanilla-skip.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train MAA: {train_maa:7.3f}' )\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Valid MAA: {valid_maa:7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.526 | Test PPL:   1.693 | Test MAA:   0.611\n",
      "test acc tensor([0.7491, 0.7141, 0.6937, 0.6869, 0.6815, 0.6797, 0.6786, 0.6946, 0.7594,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100000 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('vanilla-skip.pt'))\n",
    "\n",
    "test_loss, test_maa, test_acc = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test MAA: {test_maa:7.3f}')\n",
    "print('test acc',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = np.load(open('data/track_embeddings_w2v.npy','rb')).astype('double')\n",
    "track_embeddings = torch.from_numpy(normalize(track_embeddings,axis=0)).double()\n",
    "# track_embeddings = torch.from_numpy(normalize(track_embeddings,axis=0)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103910, 126])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSpotify(Dataset):\n",
    "    \n",
    "    def __init__(self, tracks, skips, transform=None):\n",
    "        self.tracks = tracks\n",
    "        self.skips = skips\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tracks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        features = self.tracks[index]\n",
    "        label = self.skips[index]\n",
    "        \n",
    "        features_new = []\n",
    "        for x,y in zip(features[:10],label[:10]):\n",
    "            if y == 1:\n",
    "                features_new.append(x)\n",
    "            else:\n",
    "                features_new.append(0)\n",
    "        features_new.extend(features[10:])\n",
    "\n",
    "        features = features_new\n",
    "\n",
    "#         return np.array(features[:10]), np.array(features[10:])\n",
    "    \n",
    "        return np.array(features), np.array(label)\n",
    "#         return torch.from_numpy(np.array(features)), torch.from_numpy(np.array(label))\n",
    "    \n",
    "\n",
    "train_dataset = DatasetSpotify(train_tracks,train_skips)\n",
    "valid_dataset = DatasetSpotify(valid_tracks,valid_skips)\n",
    "test_dataset = DatasetSpotify(test_tracks,test_skips)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arorasagar/miniconda3/envs/cs7643-a1/lib/python3.8/site-packages/torch/nn/modules/rnn.py:57: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq1(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(103910, 126)\n",
       "    (rnn): GRU(126, 512, dropout=0.5, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(103910, 126)\n",
       "    (rnn): GRU(126, 512, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(track_vocab) \n",
    "OUTPUT_DIM = 2\n",
    "# OUTPUT_DIM = len(track_vocab) \n",
    "\n",
    "ENC_EMB_DIM = track_embeddings.shape[1]\n",
    "DEC_EMB_DIM = track_embeddings.shape[1]\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq1(enc, dec, device).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# print(next(train_iter)[0])\n",
    "\n",
    "# x,y = next(train_iter)\n",
    "# model.forward(x.permute(1,0),y.permute(1,0)).shape\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 20s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.730 | Train MAA:   0.602\n",
      "\t Val. Loss: 0.526 |  Val. PPL:   1.693 | Valid MAA:   0.611\n",
      "Epoch: 02 | Time: 2m 47s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694 | Train MAA:   0.610\n",
      "\t Val. Loss: 0.524 |  Val. PPL:   1.688 | Valid MAA:   0.613\n",
      "Epoch: 03 | Time: 2m 51s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.691 | Train MAA:   0.612\n",
      "\t Val. Loss: 0.523 |  Val. PPL:   1.686 | Valid MAA:   0.614\n",
      "Epoch: 04 | Time: 2m 55s\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.689 | Train MAA:   0.612\n",
      "\t Val. Loss: 0.524 |  Val. PPL:   1.688 | Valid MAA:   0.610\n",
      "Epoch: 05 | Time: 3m 27s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686 | Train MAA:   0.613\n",
      "\t Val. Loss: 0.521 |  Val. PPL:   1.683 | Valid MAA:   0.614\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_maa =  train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_maa, acc_at_k = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'vanilla-w2v.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train MAA: {train_maa:7.3f}' )\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Valid MAA: {valid_maa:7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.524 | Test PPL:   1.689 | Test MAA:   0.612\n",
      "acc at k tensor([0.7561, 0.7153, 0.6945, 0.6881, 0.6813, 0.6775, 0.6776, 0.6975, 0.7594,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('vanilla-w2v.pt'))\n",
    "\n",
    "test_loss, test_maa, acc_at_k = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test MAA: {test_maa:7.3f}')\n",
    "print('acc at k',acc_at_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(track_embeddings)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout = dropout, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src)).float()\n",
    "        \n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        a,b,c = hidden.shape\n",
    "        \n",
    "        return hidden[-1,:,:].view(1,b,c)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(track_embeddings)\n",
    "\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout = dropout, bidirectional=False)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input)).float()\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2Seq1(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        split_index = math.floor(src.shape[0]//2)\n",
    "\n",
    "        seq_to_encode = src[0:split_index,:]\n",
    "        seq_to_decode = src[split_index:,:]\n",
    "        targets_of_decoded = trg\n",
    "        \n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = targets_of_decoded.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden = self.encoder(seq_to_encode)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        input = trg[split_index-1,:]\n",
    "        \n",
    "        for t in range(trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "#             teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 256, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = DatasetSpotify(train_tracks,train_skips)\n",
    "valid_dataset = DatasetSpotify(valid_tracks,valid_skips)\n",
    "test_dataset = DatasetSpotify(test_tracks,test_skips)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "\n",
    "INPUT_DIM = len(track_vocab) \n",
    "OUTPUT_DIM = 2\n",
    "# OUTPUT_DIM = len(track_vocab) \n",
    "\n",
    "ENC_EMB_DIM = track_embeddings.shape[1]\n",
    "DEC_EMB_DIM = track_embeddings.shape[1]\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq1(enc, dec, device).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# print(next(train_iter)[0])\n",
    "\n",
    "x,y = next(train_iter)\n",
    "model.forward(x.permute(1,0),y.permute(1,0)).shape\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 3m 11s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.730 | Train MAA:   0.595\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.693 | Valid MAA:   0.607\n",
      "Epoch: 02 | Time: 3m 31s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694 | Train MAA:   0.606\n",
      "\t Val. Loss: 0.525 |  Val. PPL:   1.690 | Valid MAA:   0.610\n",
      "Epoch: 03 | Time: 3m 26s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690 | Train MAA:   0.611\n",
      "\t Val. Loss: 0.524 |  Val. PPL:   1.689 | Valid MAA:   0.611\n",
      "Epoch: 04 | Time: 3m 26s\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.688 | Train MAA:   0.612\n",
      "\t Val. Loss: 0.522 |  Val. PPL:   1.686 | Valid MAA:   0.615\n",
      "Epoch: 05 | Time: 3m 40s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686 | Train MAA:   0.613\n",
      "\t Val. Loss: 0.520 |  Val. PPL:   1.682 | Valid MAA:   0.615\n",
      "| Test Loss: 0.523 | Test PPL:   1.688 | Test MAA:   0.613\n",
      "acc at k tensor([0.7617, 0.7158, 0.6936, 0.6875, 0.6811, 0.6792, 0.6795, 0.6966, 0.7561,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_maa =  train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_maa, _ = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'vanilla-w2v-gru1.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train MAA: {train_maa:7.3f}' )\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Valid MAA: {valid_maa:7.3f}')\n",
    "\n",
    "BATCH_SIZE = 100000 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('vanilla-w2v-GRU1.pt'))\n",
    "\n",
    "test_loss, test_maa, acc_at_k = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test MAA: {test_maa:7.3f}')\n",
    "print('acc at k',acc_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = np.load(open('data/track_embeddings_w2v.npy','rb')).astype('double')\n",
    "track_embeddings = torch.from_numpy(normalize(track_embeddings,axis=0)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.630 | Test PPL:   1.878 | Test MAA:   0.565\n",
      "Test acc at k  tensor([0.7206, 0.6961, 0.6793, 0.6708, 0.6610, 0.6551, 0.6494, 0.6397, 0.6358,\n",
      "        0.6296])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100000\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('vanilla-w2v-GRU1.pt'))\n",
    "\n",
    "test_loss, test_maa, test_acc = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test MAA: {test_maa:7.3f}')\n",
    "print('Test acc at k ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings = np.load(open('data/track_embedding.npy','rb')).astype('double')\n",
    "\n",
    "track_embeddings = torch.from_numpy(normalize(track_embeddings,axis=0)).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([103910, 26])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(track_embeddings)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src)).float()\n",
    "        \n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(track_embeddings)\n",
    "\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input)).float()\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "\n",
    "import math\n",
    "class Seq2Seq1(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        split_index = math.floor(src.shape[0]//2)\n",
    "\n",
    "        seq_to_encode = src[0:split_index,:]\n",
    "        seq_to_decode = src[split_index:,:]\n",
    "        targets_of_decoded = trg\n",
    "        \n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = targets_of_decoded.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(seq_to_encode)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        input = trg[split_index-1,:]\n",
    "        \n",
    "        for t in range(trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "#             teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = top1\n",
    "            if teacher_forcing_ratio == 1:\n",
    "                input =  trg[t,:]\n",
    "\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSpotify(Dataset):\n",
    "    \n",
    "    def __init__(self, tracks, skips, transform=None):\n",
    "        self.tracks = tracks\n",
    "        self.skips = skips\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tracks)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        features = self.tracks[index]\n",
    "        label = self.skips[index]\n",
    "        \n",
    "        features_new = []\n",
    "        for x,y in zip(features[:10],label[:10]):\n",
    "            if y == 1:\n",
    "                features_new.append(x)\n",
    "            else:\n",
    "                features_new.append(0)\n",
    "        features_new.extend(features[10:])\n",
    "\n",
    "        features = features_new\n",
    "\n",
    "        return np.array(features), np.array(features)\n",
    "    \n",
    "        return np.array(features), np.array(label)\n",
    "#         return torch.from_numpy(np.array(features)), torch.from_numpy(np.array(label))\n",
    "    \n",
    "\n",
    "train_dataset = DatasetSpotify(train_tracks,train_skips)\n",
    "valid_dataset = DatasetSpotify(valid_tracks,valid_skips)\n",
    "test_dataset = DatasetSpotify(test_tracks,test_skips)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256 \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arorasagar/miniconda3/envs/cs7643-a1/lib/python3.8/site-packages/torch/nn/modules/rnn.py:57: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = DatasetSpotify(train_tracks,train_skips)\n",
    "valid_dataset = DatasetSpotify(valid_tracks,valid_skips)\n",
    "test_dataset = DatasetSpotify(test_tracks,test_skips)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "\n",
    "INPUT_DIM = len(track_vocab) \n",
    "OUTPUT_DIM = 2\n",
    "OUTPUT_DIM = len(track_vocab) \n",
    "\n",
    "ENC_EMB_DIM = track_embeddings.shape[1]\n",
    "DEC_EMB_DIM = track_embeddings.shape[1]\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq1(enc, dec, device).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# print(next(train_iter)[0])\n",
    "\n",
    "x,y = next(train_iter)\n",
    "# model.forward(x.permute(1,0),y.permute(1,0),1).shape\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    total_maa = 0\n",
    "    \n",
    "    for i, (x,y) in enumerate(iterator):\n",
    "        \n",
    "        src = x.permute(1,0)\n",
    "#         print('shape is',src.shape)\n",
    "        trg = y.permute(1,0)\n",
    "#         print('shape is',src.shape, trg.shape)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        split_index = math.floor(src.shape[0]//2)\n",
    "\n",
    "       \n",
    "        trg = trg[split_index:,:]\n",
    "\n",
    "        \n",
    "        output = model(src, trg, 1)\n",
    "#         print('....',output.shape, src.shape, trg.shape)\n",
    "\n",
    "#         print('....',output.permute(1,0,2).shape)\n",
    "        maa = mean_average_accuracy(output.permute(1,0,2),trg.permute(1,0))\n",
    "        if i % 10 == 0:\n",
    "            print( i, epoch_loss / (i+1), total_maa / (i+1))\n",
    "\n",
    "\n",
    "        \n",
    "#         print('now')\n",
    "#         print(output.shape, trg.shape)\n",
    "#         print(output[:,0,:],trg[:,0])\n",
    "#         print(output[:,1,:],trg[:,1])\n",
    "#         print(output[:,2,:],trg[:,2])\n",
    "#         print(output[:,3,:],trg[:,3])\n",
    "#         break\n",
    "        \n",
    "\n",
    "        \n",
    "#         if i % 100 == 0:\n",
    "#             print('Loss',i,epoch_loss)\n",
    "#             print('mean avg accuracy', mean_average_accuracy(output.permute(1,0,2)[10:],trg.permute(1,0)[10:]))\n",
    "\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "#         print('here initial output shape is ',output.shape)\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "#         print('output shapes are',output.shape, trg.shape)\n",
    "\n",
    "        trg = trg[1:].reshape(-1)\n",
    "    \n",
    "#         print('output is ',output)\n",
    "#         print('trg is ',trg)\n",
    "\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "#         print(maa, loss)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        total_maa += maa\n",
    "\n",
    "        \n",
    "    return epoch_loss / len(iterator), total_maa / len(iterator)\n",
    "\n",
    "# def evaluate(model, iterator, criterion):\n",
    "    \n",
    "#     model.eval()\n",
    "    \n",
    "#     epoch_loss = 0\n",
    "#     total_maa = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "    \n",
    "#         for i, (x,y) in enumerate(iterator):\n",
    "\n",
    "\n",
    "#             src = x.permute(1,0)\n",
    "#             trg = y.permute(1,0)\n",
    "#             split_index = math.floor(src.shape[0]//2)\n",
    "\n",
    "#             trg = trg[split_index:,:]\n",
    "\n",
    "            \n",
    "\n",
    "#             output = model(src, trg, 0) #turn off teacher forcing\\\n",
    "#             max_output = torch.argmax(output,dim=2)\n",
    "#             maa = mean_average_accuracy(output.permute(1,0,2),trg.permute(1,0))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             #trg = [trg len, batch size]\n",
    "#             #output = [trg len, batch size, output dim]\n",
    "\n",
    "#             output_dim = output.shape[-1]\n",
    "            \n",
    "#             output = output[1:].view(-1, output_dim)\n",
    "#             trg = trg[1:].reshape(-1)\n",
    "\n",
    "#             #trg = [(trg len - 1) * batch size]\n",
    "#             #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "#             loss = criterion(output, trg)\n",
    "            \n",
    "#             epoch_loss += loss.item()\n",
    "#             total_maa += maa\n",
    "        \n",
    "#     return epoch_loss / len(iterator), total_maa / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0.0\n",
      "10 7.577510833740234 tensor(0.0354)\n",
      "20 7.906418209984189 tensor(0.0358)\n",
      "30 8.016829183024745 tensor(0.0364)\n",
      "40 8.067983371455496 tensor(0.0380)\n",
      "50 8.082865621529374 tensor(0.0397)\n",
      "60 8.116044200834681 tensor(0.0403)\n",
      "70 8.139649820999361 tensor(0.0406)\n",
      "80 8.158368004692925 tensor(0.0407)\n",
      "90 8.165168830326625 tensor(0.0412)\n",
      "100 8.16097086254913 tensor(0.0419)\n",
      "110 8.171688608221105 tensor(0.0421)\n",
      "120 8.173308348852741 tensor(0.0424)\n",
      "130 8.171818292777957 tensor(0.0429)\n",
      "140 8.175747090197625 tensor(0.0432)\n",
      "150 8.174734510333332 tensor(0.0435)\n",
      "160 8.175497262374215 tensor(0.0435)\n",
      "170 8.175672458626373 tensor(0.0436)\n",
      "180 8.170389560046116 tensor(0.0439)\n",
      "190 8.169779917332514 tensor(0.0441)\n",
      "200 8.16382630666097 tensor(0.0445)\n",
      "210 8.161162760585405 tensor(0.0448)\n",
      "220 8.15874141158022 tensor(0.0450)\n",
      "230 8.157646682871368 tensor(0.0452)\n",
      "240 8.156693589143238 tensor(0.0454)\n",
      "250 8.15181381483952 tensor(0.0457)\n",
      "260 8.150057376116171 tensor(0.0460)\n",
      "270 8.148340698537792 tensor(0.0462)\n",
      "280 8.14864322723443 tensor(0.0463)\n",
      "290 8.144288287539663 tensor(0.0464)\n",
      "300 8.143637321320087 tensor(0.0466)\n",
      "310 8.141806176237738 tensor(0.0469)\n",
      "320 8.141532963310075 tensor(0.0468)\n",
      "330 8.139949536395577 tensor(0.0470)\n",
      "340 8.139096577496122 tensor(0.0471)\n",
      "350 8.134673064250892 tensor(0.0475)\n",
      "360 8.129942170140486 tensor(0.0477)\n",
      "370 8.127061301485869 tensor(0.0480)\n",
      "380 8.121469224844704 tensor(0.0482)\n",
      "390 8.118676439270644 tensor(0.0485)\n",
      "Epoch: 01 | Time: 82m 39s\n",
      "\tTrain Loss: 8.139 | Train PPL: 3424.304 | Train MAA:   0.049\n",
      "\t Val. Loss: 11.477 |  Val. PPL: 96497.075 | Valid MAA:   0.006\n",
      "0 0.0 0.0\n",
      "10 7.0739617781205615 tensor(0.0545)\n",
      "20 7.388914130982899 tensor(0.0580)\n",
      "30 7.516727801292173 tensor(0.0578)\n",
      "40 7.577120862356046 tensor(0.0579)\n",
      "50 7.59304666519165 tensor(0.0593)\n",
      "60 7.617180628854721 tensor(0.0589)\n",
      "70 7.631034239916734 tensor(0.0588)\n",
      "80 7.657456509860945 tensor(0.0583)\n",
      "90 7.672894891801771 tensor(0.0581)\n",
      "100 7.682267245679799 tensor(0.0586)\n",
      "110 7.692768328898662 tensor(0.0581)\n",
      "120 7.695762590928511 tensor(0.0585)\n",
      "130 7.693530068142723 tensor(0.0587)\n",
      "140 7.691207601668987 tensor(0.0592)\n",
      "150 7.690871702914206 tensor(0.0596)\n",
      "160 7.6915175722252505 tensor(0.0599)\n",
      "170 7.687317399253622 tensor(0.0601)\n",
      "180 7.684759432439646 tensor(0.0605)\n",
      "190 7.680623491397078 tensor(0.0610)\n",
      "200 7.677235534535119 tensor(0.0611)\n",
      "210 7.676356042165891 tensor(0.0612)\n",
      "220 7.670540740587053 tensor(0.0620)\n",
      "230 7.667609542995304 tensor(0.0623)\n",
      "240 7.668499980230054 tensor(0.0625)\n",
      "250 7.66464599290217 tensor(0.0628)\n",
      "260 7.6625239090901225 tensor(0.0629)\n",
      "270 7.658429576901932 tensor(0.0632)\n",
      "280 7.659619416206333 tensor(0.0631)\n",
      "290 7.6609967975682 tensor(0.0629)\n",
      "300 7.659391468149483 tensor(0.0631)\n",
      "310 7.655858154848841 tensor(0.0634)\n",
      "320 7.6536266603202465 tensor(0.0635)\n",
      "330 7.650415305284575 tensor(0.0637)\n",
      "340 7.648819963953013 tensor(0.0638)\n",
      "350 7.643297949407855 tensor(0.0640)\n",
      "360 7.639005192098855 tensor(0.0643)\n",
      "370 7.638780688982447 tensor(0.0642)\n",
      "380 7.632462288763892 tensor(0.0646)\n",
      "390 7.629013874951531 tensor(0.0648)\n",
      "Epoch: 02 | Time: 81m 13s\n",
      "\tTrain Loss: 7.648 | Train PPL: 2096.337 | Train MAA:   0.065\n",
      "\t Val. Loss: 11.756 |  Val. PPL: 127490.289 | Valid MAA:   0.006\n",
      "0 0.0 0.0\n",
      "10 6.6460185484452685 tensor(0.0652)\n",
      "20 6.943403925214495 tensor(0.0685)\n",
      "30 7.045830757387223 tensor(0.0709)\n",
      "40 7.123709073880824 tensor(0.0686)\n",
      "50 7.157711244096943 tensor(0.0694)\n",
      "60 7.197101686821609 tensor(0.0687)\n",
      "70 7.215762386859303 tensor(0.0690)\n",
      "80 7.220148198398543 tensor(0.0702)\n",
      "90 7.224940913064139 tensor(0.0709)\n",
      "100 7.2289120041497865 tensor(0.0711)\n",
      "110 7.230761158573735 tensor(0.0715)\n",
      "120 7.226983590559526 tensor(0.0718)\n",
      "130 7.221267459956744 tensor(0.0725)\n",
      "140 7.219732872983243 tensor(0.0732)\n",
      "150 7.22232517027697 tensor(0.0732)\n",
      "160 7.224934805994448 tensor(0.0732)\n",
      "170 7.227449154993247 tensor(0.0734)\n",
      "180 7.230997006537506 tensor(0.0734)\n",
      "190 7.230326327978005 tensor(0.0736)\n",
      "200 7.229784460210088 tensor(0.0739)\n",
      "210 7.228529460057263 tensor(0.0742)\n",
      "220 7.227545660545384 tensor(0.0743)\n",
      "230 7.2270057707121875 tensor(0.0744)\n",
      "240 7.2274871050569525 tensor(0.0744)\n",
      "250 7.226241867855726 tensor(0.0745)\n",
      "260 7.221708516964967 tensor(0.0749)\n",
      "270 7.220596565091742 tensor(0.0751)\n",
      "280 7.218483353000519 tensor(0.0752)\n",
      "290 7.216739534922072 tensor(0.0754)\n",
      "300 7.2170439273416 tensor(0.0755)\n",
      "310 7.215652421356397 tensor(0.0756)\n",
      "320 7.211973731020158 tensor(0.0760)\n",
      "330 7.213133116142988 tensor(0.0759)\n",
      "340 7.210621242299458 tensor(0.0761)\n",
      "350 7.207531268780048 tensor(0.0761)\n",
      "360 7.205079507959847 tensor(0.0763)\n",
      "370 7.203923688101962 tensor(0.0763)\n",
      "380 7.201990227686764 tensor(0.0764)\n",
      "390 7.199771576220422 tensor(0.0766)\n",
      "Epoch: 03 | Time: 81m 53s\n",
      "\tTrain Loss: 7.218 | Train PPL: 1363.083 | Train MAA:   0.077\n",
      "\t Val. Loss: 11.537 |  Val. PPL: 102470.028 | Valid MAA:   0.010\n",
      "0 0.0 0.0\n",
      "10 6.22630652514371 tensor(0.0751)\n",
      "20 6.486717451186407 tensor(0.0799)\n",
      "30 6.608800811152304 tensor(0.0800)\n",
      "40 6.673436990598353 tensor(0.0810)\n",
      "50 6.709328034344842 tensor(0.0817)\n",
      "60 6.734927107076176 tensor(0.0817)\n",
      "70 6.761500358581543 tensor(0.0813)\n",
      "80 6.780962843953827 tensor(0.0812)\n",
      "90 6.790610769292811 tensor(0.0816)\n",
      "100 6.796233488781618 tensor(0.0818)\n",
      "110 6.7998978769457015 tensor(0.0822)\n",
      "120 6.797755379322147 tensor(0.0828)\n",
      "130 6.805766779047842 tensor(0.0825)\n",
      "140 6.812045002660008 tensor(0.0824)\n",
      "150 6.820176301413024 tensor(0.0821)\n",
      "160 6.82279146087836 tensor(0.0824)\n",
      "170 6.821247039482608 tensor(0.0828)\n",
      "180 6.824420986913186 tensor(0.0830)\n",
      "190 6.827750525549444 tensor(0.0831)\n",
      "200 6.829317491445968 tensor(0.0831)\n",
      "210 6.826647417240233 tensor(0.0834)\n",
      "220 6.831565671377053 tensor(0.0833)\n",
      "230 6.834330953044809 tensor(0.0833)\n",
      "240 6.835252987398646 tensor(0.0836)\n",
      "250 6.835652605945845 tensor(0.0837)\n",
      "260 6.837924778233086 tensor(0.0838)\n",
      "270 6.8388298763120305 tensor(0.0837)\n",
      "280 6.834701658567924 tensor(0.0841)\n",
      "290 6.833260303510423 tensor(0.0843)\n",
      "300 6.833074249698474 tensor(0.0845)\n",
      "310 6.831004447875682 tensor(0.0847)\n",
      "320 6.831264883558327 tensor(0.0846)\n",
      "330 6.8313234205332405 tensor(0.0847)\n",
      "340 6.830566003525362 tensor(0.0848)\n",
      "350 6.830747498406304 tensor(0.0849)\n",
      "360 6.829927952996251 tensor(0.0850)\n",
      "370 6.829030534327834 tensor(0.0850)\n",
      "380 6.827878743019004 tensor(0.0851)\n",
      "390 6.826963521025675 tensor(0.0852)\n",
      "Epoch: 04 | Time: 83m 58s\n",
      "\tTrain Loss: 6.844 | Train PPL: 938.250 | Train MAA:   0.085\n",
      "\t Val. Loss: 12.042 |  Val. PPL: 169725.399 | Valid MAA:   0.013\n",
      "0 0.0 0.0\n",
      "10 5.891067201440984 tensor(0.0825)\n",
      "20 6.213919685000465 tensor(0.0852)\n",
      "30 6.297497933910739 tensor(0.0880)\n",
      "40 6.351556277856594 tensor(0.0888)\n",
      "50 6.3728383382161455 tensor(0.0906)\n",
      "60 6.415993299640593 tensor(0.0897)\n",
      "70 6.421440930433676 tensor(0.0905)\n",
      "80 6.436580139913676 tensor(0.0904)\n",
      "90 6.452383361019931 tensor(0.0902)\n",
      "100 6.4661499391687975 tensor(0.0899)\n",
      "110 6.470519405227524 tensor(0.0903)\n",
      "120 6.4768253042678205 tensor(0.0903)\n",
      "130 6.474877321083127 tensor(0.0910)\n",
      "140 6.485539351794737 tensor(0.0910)\n",
      "150 6.487883567810059 tensor(0.0911)\n",
      "160 6.491935718133583 tensor(0.0911)\n",
      "170 6.494348063106425 tensor(0.0912)\n",
      "180 6.496963026773864 tensor(0.0912)\n",
      "190 6.491201657899388 tensor(0.0916)\n",
      "200 6.491758813905479 tensor(0.0917)\n",
      "210 6.488817840955834 tensor(0.0921)\n",
      "220 6.489372432501607 tensor(0.0923)\n",
      "230 6.490363992137826 tensor(0.0925)\n",
      "240 6.49137026244674 tensor(0.0925)\n",
      "250 6.494348581093716 tensor(0.0923)\n",
      "260 6.495932100376407 tensor(0.0924)\n",
      "270 6.500444067360291 tensor(0.0923)\n",
      "280 6.4996209891241215 tensor(0.0922)\n",
      "290 6.49994835575012 tensor(0.0924)\n",
      "300 6.498906176747674 tensor(0.0927)\n",
      "310 6.500548675512578 tensor(0.0928)\n",
      "320 6.499911042388726 tensor(0.0929)\n",
      "330 6.497688081689471 tensor(0.0931)\n",
      "340 6.496197441805842 tensor(0.0934)\n",
      "350 6.495352696149777 tensor(0.0935)\n",
      "360 6.4954621058752 tensor(0.0935)\n",
      "370 6.496803381372334 tensor(0.0935)\n",
      "380 6.496989446675058 tensor(0.0935)\n",
      "390 6.496418574886858 tensor(0.0935)\n",
      "Epoch: 05 | Time: 84m 47s\n",
      "\tTrain Loss: 6.512 | Train PPL: 673.239 | Train MAA:   0.094\n",
      "\t Val. Loss: 11.648 |  Val. PPL: 114467.204 | Valid MAA:   0.013\n",
      "0 0.0 0.0\n",
      "10 5.611032746054909 tensor(0.0904)\n",
      "20 5.888197739919026 tensor(0.0967)\n",
      "30 5.980562779211229 tensor(0.0987)\n",
      "40 6.060462079397062 tensor(0.0978)\n",
      "50 6.091666848051782 tensor(0.0976)\n",
      "60 6.109382441786469 tensor(0.0980)\n",
      "70 6.126458530694666 tensor(0.0981)\n",
      "80 6.141119415377394 tensor(0.0985)\n",
      "90 6.152300792736011 tensor(0.0987)\n",
      "100 6.159554453179387 tensor(0.0990)\n",
      "110 6.168468483933458 tensor(0.0992)\n",
      "120 6.172980087847749 tensor(0.0991)\n",
      "130 6.176143402361688 tensor(0.0992)\n",
      "140 6.179804233794517 tensor(0.0993)\n",
      "150 6.18262975579066 tensor(0.0997)\n",
      "160 6.193131574192402 tensor(0.0991)\n",
      "170 6.194816238001773 tensor(0.0993)\n",
      "180 6.198492187162789 tensor(0.0994)\n",
      "190 6.196295675806974 tensor(0.1000)\n",
      "200 6.201378113001733 tensor(0.0999)\n",
      "210 6.200650533793662 tensor(0.1000)\n",
      "220 6.200211425712205 tensor(0.1004)\n",
      "230 6.20158103018096 tensor(0.1004)\n",
      "240 6.202083652939539 tensor(0.1007)\n",
      "250 6.2035964057739985 tensor(0.1007)\n",
      "260 6.2088725466381085 tensor(0.1006)\n",
      "270 6.209240865883352 tensor(0.1007)\n",
      "280 6.205831800915592 tensor(0.1010)\n",
      "290 6.2063480636098545 tensor(0.1010)\n",
      "300 6.207871367368983 tensor(0.1010)\n",
      "310 6.2070744643257365 tensor(0.1012)\n",
      "320 6.208513594119348 tensor(0.1013)\n",
      "330 6.209446885434522 tensor(0.1014)\n",
      "340 6.210416571485681 tensor(0.1015)\n",
      "350 6.210618514960308 tensor(0.1016)\n",
      "360 6.209907990083139 tensor(0.1016)\n",
      "370 6.211575080120981 tensor(0.1015)\n",
      "380 6.209605929106865 tensor(0.1019)\n",
      "390 6.209301703421356 tensor(0.1020)\n",
      "Epoch: 06 | Time: 86m 41s\n",
      "\tTrain Loss: 6.225 | Train PPL: 505.047 | Train MAA:   0.102\n",
      "\t Val. Loss: 11.840 |  Val. PPL: 138623.596 | Valid MAA:   0.013\n",
      "0 0.0 0.0\n",
      "10 5.4345034686001865 tensor(0.0994)\n",
      "20 5.648737793877011 tensor(0.1063)\n",
      "30 5.74957626096664 tensor(0.1079)\n",
      "40 5.800446940631401 tensor(0.1088)\n",
      "50 5.821641996795056 tensor(0.1102)\n",
      "60 5.849951439216489 tensor(0.1104)\n",
      "70 5.853898686422428 tensor(0.1115)\n",
      "80 5.863963280195072 tensor(0.1116)\n",
      "90 5.869154070759867 tensor(0.1117)\n",
      "100 5.8778693463542675 tensor(0.1118)\n",
      "110 5.888899566890957 tensor(0.1113)\n",
      "120 5.894153866886107 tensor(0.1115)\n",
      "130 5.897600348669154 tensor(0.1118)\n",
      "140 5.90469321798771 tensor(0.1118)\n",
      "150 5.908453770820668 tensor(0.1118)\n",
      "160 5.91257241349783 tensor(0.1117)\n",
      "170 5.918772951204177 tensor(0.1116)\n",
      "180 5.92608755069543 tensor(0.1114)\n",
      "190 5.931899330378827 tensor(0.1110)\n",
      "200 5.930288044374381 tensor(0.1113)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('vanilla-sequence.pt'))\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_maa =  train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_maa, _ = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'vanilla-sequence.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train MAA: {train_maa:7.3f}' )\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Valid MAA: {valid_maa:7.3f}')\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('vanilla-sequence.pt'))\n",
    "\n",
    "test_loss, test_maa, acc_at_k = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test MAA: {test_maa:7.3f}')\n",
    "print('acc at k', acc_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 11.905 | Test PPL: 147955.750 | Test MAA:   0.006\n",
      "acc at k tensor([0.0085, 0.0099, 0.0067, 0.0077, 0.0066, 0.0077, 0.0055, 0.0065, 0.0054,\n",
      "        0.0055])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_iter, valid_iter, test_iter = iter(train_loader),iter(valid_loader),iter(test_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('vanilla-sequence.pt'))\n",
    "\n",
    "test_loss, test_maa, acc_at_k = evaluate(model, test_iter, criterion)\n",
    "\n",
    "\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test MAA: {test_maa:7.3f}')\n",
    "print('acc at k', acc_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs7643-a1] *",
   "language": "python",
   "name": "conda-env-cs7643-a1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
